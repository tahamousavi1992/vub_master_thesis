{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YQQ8vSuzV36n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1ee3983-ded9-4ae8-db6a-ec3dc943ff64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run on GPU: True\n"
          ]
        }
      ],
      "source": [
        "# !pip install pyitlib\n",
        "# from pyitlib import discrete_random_variable as drv\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import os\n",
        "from PIL import Image\n",
        "import operator\n",
        "from shutil import copyfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import math\n",
        "from skimage.measure import shannon_entropy\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "print('Run on GPU: ' + str(train_on_gpu))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S_j_jMKjXuMo"
      },
      "outputs": [],
      "source": [
        "# create Dataset object to support batch training\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, features, labels, transform):\n",
        "        self.features = features             \n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if self.transform is None:\n",
        "            return (self.features[idx], self.labels[idx])\n",
        "        else:\n",
        "            return (self.transform(self.features[idx]), self.labels[idx])\n",
        "\n",
        "            \n",
        "class Cutout(object):\n",
        "    \"\"\"Randomly mask out one or more patches from an image.\n",
        "    Args:\n",
        "        n_holes (int): Number of patches to cut out of each image.\n",
        "        length (int): The length (in pixels) of each square patch.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_holes, length):\n",
        "        self.n_holes = n_holes\n",
        "        self.length = length\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (Tensor): Tensor image of size (C, H, W).\n",
        "        Returns:\n",
        "            Tensor: Image with n_holes of dimension length x length cut out of it.\n",
        "        \"\"\"\n",
        "        h = img.size(1)\n",
        "        w = img.size(2)\n",
        "\n",
        "        mask = np.ones((h, w), np.float32)\n",
        "\n",
        "        for n in range(self.n_holes):\n",
        "            y = np.random.randint(h)\n",
        "            x = np.random.randint(w)\n",
        "\n",
        "            y1 = np.clip(y - self.length // 2, 0, h)\n",
        "            y2 = np.clip(y + self.length // 2, 0, h)\n",
        "            x1 = np.clip(x - self.length // 2, 0, w)\n",
        "            x2 = np.clip(x + self.length // 2, 0, w)\n",
        "\n",
        "            mask[y1: y2, x1: x2] = 0.\n",
        "\n",
        "        mask = torch.from_numpy(mask)\n",
        "        mask = mask.expand_as(img)\n",
        "        img = img * mask\n",
        "\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WWuYJoXN2mCY"
      },
      "outputs": [],
      "source": [
        "normalize = torchvision.transforms.transforms.Normalize(\n",
        "    mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
        "    std=[x / 255.0 for x in [63.0, 62.1, 66.7]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "q_uaN5Tn0Jw-"
      },
      "outputs": [],
      "source": [
        "def calculate_entropy(img):\n",
        "    # convert image to gray-scale\n",
        "    gray_image = img.convert('L')\n",
        "    gray_image = np.array(gray_image)\n",
        "    gray_image = gray_image / 255.0\n",
        "\n",
        "    # Compute histogram of image pixel intensities\n",
        "    hist = np.histogram(gray_image, bins=256)[0]\n",
        "    \n",
        "    # Normalize histogram to compute probabilities\n",
        "    probabilities = hist / np.sum(hist)\n",
        "    \n",
        "    # Compute entropy\n",
        "    entropy = -np.sum(probabilities * np.log2(probabilities + np.finfo(float).eps))\n",
        "\n",
        "    return entropy\n",
        "\n",
        "def sort_by_entropy(origin_dataset):\n",
        "    results = []\n",
        "    entropy_values = []\n",
        "    for item in origin_dataset:\n",
        "        entropy = calculate_entropy(item[0])\n",
        "        results.append([item, entropy])\n",
        "        entropy_values.append(entropy)\n",
        "\n",
        "    results = sorted(results, key=lambda x: x[1])\n",
        "    return results, entropy_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "b7RUTfNyw2c2"
      },
      "outputs": [],
      "source": [
        "def get_dataloader(trainset, testset, percentages):\n",
        "    if sum(percentages) != 1:\n",
        "      raise ValueError(\"Percentages do not add up to 100\")\n",
        "\n",
        "    transform_train = torchvision.transforms.Compose([\n",
        "      torchvision.transforms.RandomCrop(32, padding=4),\n",
        "      torchvision.transforms.RandomHorizontalFlip(),\n",
        "      torchvision.transforms.ToTensor(),\n",
        "      normalize,\n",
        "      Cutout(n_holes=1, length=16)\n",
        "    ])\n",
        "\n",
        "    trainset, entropy_values = sort_by_entropy(trainset)\n",
        "\n",
        "    trainset_len = len(trainset)\n",
        "    chunk1 = int(trainset_len * percentages[0])\n",
        "    chunk2 = int(trainset_len * percentages[1])\n",
        "    chunk3 = trainset_len - chunk2 - chunk1\n",
        " \n",
        "\n",
        "    chunk_count = 3\n",
        "    train_dl_arr = []\n",
        "\n",
        "    x_train = [item[0][0] for item in trainset[0:chunk1]]\n",
        "    y_train = [item[0][1] for item in trainset[0:chunk1]]\n",
        "    train_dl_arr.append(torch.utils.data.DataLoader(Dataset(x_train, y_train, transform_train), batch_size= 128, shuffle=True))\n",
        "\n",
        "    x_train = [item[0][0] for item in trainset[chunk1:(chunk1 + chunk2)]]\n",
        "    y_train = [item[0][1] for item in trainset[chunk1:(chunk1 + chunk2)]]\n",
        "    train_dl_arr.append(torch.utils.data.DataLoader(Dataset(x_train, y_train, transform_train), batch_size= 128, shuffle=True))\n",
        "\n",
        "    x_train = [item[0][0] for item in trainset[(chunk1 + chunk2):]]\n",
        "    y_train = [item[0][1] for item in trainset[(chunk1 + chunk2):]]\n",
        "    train_dl_arr.append(torch.utils.data.DataLoader(Dataset(x_train, y_train, transform_train), batch_size= 128, shuffle=True))\n",
        " \n",
        "    # last chunk load all the data\n",
        "    x_train = [item[0][0] for item in trainset]\n",
        "    y_train = [item[0][1] for item in trainset]\n",
        "    train_dl_arr.append(torch.utils.data.DataLoader(Dataset(x_train, y_train, transform_train), batch_size= 128, shuffle=True))\n",
        "    \n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=2500, shuffle=False)\n",
        "    \n",
        "    return train_dl_arr, testloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "B8mU6aSb3qpt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a7aa01-1dcc-4723-8ecb-c41a68dc886f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:14<00:00, 11943629.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n",
            "number of train images:50000\n",
            "number of test images:10000\n"
          ]
        }
      ],
      "source": [
        "transform_test = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root= 'data', train=True, download=True)\n",
        "testset = torchvision.datasets.CIFAR10(root= 'data', train=False, download=True, transform=transform_test)\n",
        "print('number of train images:' + str(len(trainset)))\n",
        "print('number of test images:' + str(len(testset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OlyhQTb0SxZ2"
      },
      "outputs": [],
      "source": [
        "'''ResNet18/34/50/101/152 in Pytorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(in_planes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = conv3x3(3,64)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18(num_classes=10):\n",
        "    return ResNet(BasicBlock, [2,2,2,2], num_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DBV-aR26lspA"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model,data):\n",
        "    return model(data).cpu().numpy().argmax(axis=1)    \n",
        " \n",
        "def poly_learning_rate(base_lr, curr_iter, max_iter, power=0.9):\n",
        "    \"\"\"poly learning rate policy\"\"\"\n",
        "    lr = base_lr * (1 - float(curr_iter) / max_iter) ** power\n",
        "    return lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "wPQRmXPIEJ_-"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "def log_info(i_epoch, chunk_index):\n",
        "      message = ' ::: epoch: ' + str(i_epoch)\n",
        "      message += ' chunk: ' + str(chunk_index)\n",
        "      return message\n",
        "      \n",
        "torch.manual_seed(5)\n",
        "torch.cuda.manual_seed(5)\n",
        "\n",
        "def trian_model(model, optimizer, learning_rate, epoch_num, train_sets, testloader, epoch_per_chunk):\n",
        "    if train_on_gpu:\n",
        "       model.cuda()\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    criterion.__init__(reduce=False)\n",
        "    \n",
        "    optimizer = optimizer(model.parameters(),lr=learning_rate, momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
        "    val_accs = np.zeros(epoch_num)\n",
        "\n",
        "    lr_arr = []\n",
        "    \n",
        "    info_message = ''\n",
        "    chunk_index = 0\n",
        " \n",
        "    traversed_chuncks = [0]\n",
        "    p_bar = tqdm(range(epoch_num))\n",
        "\n",
        "    count_complext_lr = 0\n",
        "    current_lr = 0\n",
        "    # Before training loop, initialize a counter for the total number of iterations\n",
        "    total_iterations = 0\n",
        "\n",
        "    for i_epoch in p_bar:\n",
        "        model.train()\n",
        "\n",
        "        # change chunk index.\n",
        "        if (i_epoch + 1) > sum(epoch_per_chunk[0:(chunk_index+1)]):\n",
        "          total_iterations = 0\n",
        "          chunk_index += 1\n",
        "          traversed_chuncks.append(chunk_index)\n",
        "          info_message += log_info((i_epoch+1), chunk_index)\n",
        "\n",
        "        train_set = train_sets[chunk_index]\n",
        "        # max_iter = epoch_num * len(train_sets[0])\n",
        "        max_iter = epoch_num * len(train_set)\n",
        "        for i_batch, (X_batch, y_batch) in enumerate(train_set):\n",
        "            # After each batch, increment the total_iterations\n",
        "            total_iterations += 1\n",
        "            if(train_on_gpu):\n",
        "                X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
        "\n",
        "            model.zero_grad()  # reset model gradients\n",
        "            output = model(X_batch)  # conduct forward pass  \n",
        "\n",
        "            loss=criterion(output, y_batch) \n",
        "\n",
        "            loss = loss.mean() # added for example_forgetting github \n",
        "            loss.backward()  # backpropogate loss to calculate gradients\n",
        " \n",
        "            new_lr = poly_learning_rate(learning_rate, total_iterations, max_iter, power=0.9)\n",
        "            if type(new_lr) == complex:\n",
        "              count_complext_lr += 1\n",
        "            else:\n",
        "              current_lr = new_lr\n",
        "              \n",
        "            for g in optimizer.param_groups:\n",
        "                 g['lr'] = current_lr\n",
        "            lr_arr.append(current_lr)\n",
        "\n",
        "            try:\n",
        "              optimizer.step()  # update model weights  \n",
        "            except:\n",
        "              print('current_iter: '+ str(total_iterations))\n",
        "              print('last max_iter: '+ str(max_iter))\n",
        "              print('last learning: '+ str(current_lr))\n",
        "              optimizer.step()  # update model weights  \n",
        "            \n",
        " \n",
        " \n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():  # no need to calculate gradients when assessing accuracy\n",
        "            for i_batch, (X_val, Y_val) in enumerate(testloader):\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                  X_val, Y_val = X_val.cuda(), Y_val.cuda()\n",
        "\n",
        "                model.eval()\n",
        "\n",
        "                pred_val = get_predictions(model, X_val)\n",
        "                total += X_val.size(0)\n",
        "                correct += (pred_val == Y_val.cpu().numpy()).sum()\n",
        "\n",
        "            val_acc = 100. * correct.item() / total\n",
        "            val_accs[i_epoch] = val_acc\n",
        "            p_bar.set_description((\"max accuracy: \" + str(val_accs.max()) + ' accuracy: ' + str(val_acc)) +\n",
        "                                  ' last lr: ' + str(current_lr) +' chunk index: ' +str(chunk_index), \n",
        "                                   refresh=True)\n",
        "            \n",
        "    print('Traversed chunks'+ str(traversed_chuncks))  \n",
        "    print(info_message)\n",
        "    print(\"count_complext_lr: \" + str(count_complext_lr))    \n",
        "    return val_accs, lr_arr\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "IqUDZkDN5P3D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "011ecf9a-3638-4e96-f7a3-962efcae5874"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of remaining train images in chunk 0:2500\n",
            "number of remaining train images in chunk 1:7500\n",
            "number of remaining train images in chunk 2:40000\n",
            "number of remaining train images in chunk 3:50000\n",
            "number of test images:10000\n"
          ]
        }
      ],
      "source": [
        "train_sets, testloader = get_dataloader(trainset, testset, [0.05,0.15,0.80]) \n",
        "print('number of remaining train images in chunk 0:' + str(len(train_sets[0].dataset)))\n",
        "print('number of remaining train images in chunk 1:' + str(len(train_sets[1].dataset)))\n",
        "print('number of remaining train images in chunk 2:' + str(len(train_sets[2].dataset)))\n",
        "print('number of remaining train images in chunk 3:' + str(len(train_sets[3].dataset)))\n",
        "print('number of test images:' + str(len(testloader.dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resNet18 = ResNet18()  \n",
        "optimizer = torch.optim.SGD\n",
        "epoch_per_chunk = [3, 7, 40, 1]\n",
        "val_acc, lr_arr = trian_model(resNet18, optimizer, 0.1, 51, train_sets, testloader, epoch_per_chunk)"
      ],
      "metadata": {
        "id": "LNjgmTlIQnOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "831Aqod5FAZN"
      },
      "outputs": [],
      "source": [
        "plt.plot(lr_arr)  \n",
        "plt.xlabel('steps')\n",
        "plt.ylabel('lr value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(val_acc)  \n",
        "plt.xlabel('steps')\n",
        "plt.ylabel('acc value')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Do-Pf5H8os8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run conventional model\n",
        "import copy\n",
        "\n",
        "def train_conventional_model(model, optimizer, learning_rate, epoch_num, train_set, testloader):\n",
        "    if train_on_gpu:\n",
        "       model.cuda()\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    criterion.__init__(reduce=False)\n",
        "    \n",
        "    optimizer = optimizer(model.parameters(),lr=learning_rate, momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
        "    val_accs = np.zeros(epoch_num)\n",
        "\n",
        "    lr_arr = []\n",
        "    \n",
        "    info_message = ''\n",
        " \n",
        "    traversed_chuncks = [0]\n",
        "    p_bar = tqdm(range(epoch_num))\n",
        " \n",
        "    count_complext_lr = 0\n",
        "    current_lr = 0\n",
        "    total_iterations = 0\n",
        "\n",
        "    for i_epoch in p_bar:\n",
        "        model.train()\n",
        "\n",
        "        max_iter = epoch_num * len(train_set)\n",
        "        for i_batch, (X_batch, y_batch) in enumerate(train_set):         \n",
        "            total_iterations += 1\n",
        "            if(train_on_gpu):\n",
        "                X_batch, y_batch = X_batch.cuda(), y_batch.cuda()\n",
        "\n",
        "            model.zero_grad()  # reset model gradients\n",
        "            output = model(X_batch)  # conduct forward pass  \n",
        "\n",
        "            loss=criterion(output, y_batch) \n",
        "\n",
        "            loss = loss.mean() # added for example_forgetting github \n",
        "            loss.backward()  # backpropogate loss to calculate gradients\n",
        "  \n",
        "            current_lr = poly_learning_rate(learning_rate, total_iterations, max_iter, power=0.9)\n",
        "              \n",
        "            for g in optimizer.param_groups:\n",
        "                 g['lr'] = current_lr\n",
        "            lr_arr.append(current_lr)\n",
        "\n",
        "            try:\n",
        "              optimizer.step()  # update model weights  \n",
        "            except:\n",
        "              print('current_iter: '+ str(total_iterations))\n",
        "              print('last max_iter: '+ str(max_iter))\n",
        "              print('last learning: '+ str(current_lr))\n",
        "              optimizer.step()  # update model weights  \n",
        "             \n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():  # no need to calculate gradients when assessing accuracy\n",
        "            for i_batch, (X_val, Y_val) in enumerate(testloader):\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                  X_val, Y_val = X_val.cuda(), Y_val.cuda()\n",
        "\n",
        "                model.eval()\n",
        "\n",
        "                pred_val = get_predictions(model, X_val)\n",
        "                total += X_val.size(0)\n",
        "                correct += (pred_val == Y_val.cpu().numpy()).sum()\n",
        "\n",
        "            val_acc = 100. * correct.item() / total\n",
        "            val_accs[i_epoch] = val_acc\n",
        "            p_bar.set_description((\"max accuracy: \" + str(val_accs.max()) + ' accuracy: ' + str(val_acc)) +\n",
        "                                  ' last lr: ' + str(current_lr), \n",
        "                                   refresh=True)\n",
        "            \n",
        "    print('Traversed chunks'+ str(traversed_chuncks))  \n",
        "    print(info_message)\n",
        "    print(\"count_complext_lr: \" + str(count_complext_lr))    \n",
        "    return val_accs, lr_arr"
      ],
      "metadata": {
        "id": "70fQZ9vJ-Uv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resNet18 = ResNet18()  \n",
        "optimizer = torch.optim.SGD\n",
        "val_acc, lr_arr = train_conventional_model(resNet18, optimizer, 0.1, 51, train_sets[3], testloader)"
      ],
      "metadata": {
        "id": "qcQ4SDtH_N3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lr_arr)  \n",
        "plt.xlabel('steps')\n",
        "plt.ylabel('lr value')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XMjW7wdhIplV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
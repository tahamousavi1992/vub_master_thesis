{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQQ8vSuzV36n"
      },
      "outputs": [],
      "source": [
        "# !pip install pyitlib\n",
        "# from pyitlib import discrete_random_variable as drv\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import os\n",
        "from PIL import Image\n",
        "import operator\n",
        "from shutil import copyfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy.random as npr\n",
        "from skimage.measure import shannon_entropy\n",
        "\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "print('Run on GPU: ' + str(train_on_gpu))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_j_jMKjXuMo"
      },
      "outputs": [],
      "source": [
        "# create Dataset object to support batch training\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, features, targets, transform):\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.transform is None:\n",
        "            return (self.features[idx], self.targets[idx])\n",
        "        else:\n",
        "            return (self.transform(self.features[idx]), self.targets[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsTyJcGVbUYH"
      },
      "outputs": [],
      "source": [
        "class Cutout(object):\n",
        "    \"\"\"Randomly mask out one or more patches from an image.\n",
        "    Args:\n",
        "        n_holes (int): Number of patches to cut out of each image.\n",
        "        length (int): The length (in pixels) of each square patch.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_holes, length):\n",
        "        self.n_holes = n_holes\n",
        "        self.length = length\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (Tensor): Tensor image of size (C, H, W).\n",
        "        Returns:\n",
        "            Tensor: Image with n_holes of dimension length x length cut out of it.\n",
        "        \"\"\"\n",
        "        h = img.size(1)\n",
        "        w = img.size(2)\n",
        "\n",
        "        mask = np.ones((h, w), np.float32)\n",
        "\n",
        "        for n in range(self.n_holes):\n",
        "            y = np.random.randint(h)\n",
        "            x = np.random.randint(w)\n",
        "\n",
        "            y1 = np.clip(y - self.length // 2, 0, h)\n",
        "            y2 = np.clip(y + self.length // 2, 0, h)\n",
        "            x1 = np.clip(x - self.length // 2, 0, w)\n",
        "            x2 = np.clip(x + self.length // 2, 0, w)\n",
        "\n",
        "            mask[y1: y2, x1: x2] = 0.\n",
        "\n",
        "        mask = torch.from_numpy(mask)\n",
        "        mask = mask.expand_as(img)\n",
        "        img = img * mask\n",
        "\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWuYJoXN2mCY"
      },
      "outputs": [],
      "source": [
        "normalize = torchvision.transforms.transforms.Normalize(\n",
        "    mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
        "    std=[x / 255.0 for x in [63.0, 62.1, 66.7]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def calculate_entropy(image, x_axis = True):\n",
        "#     grey_img = image.convert('L')\n",
        "#     grey_img = np.array(grey_img).flatten()\n",
        "#     entropy = drv.entropy(grey_img, Alphabet_X=np.arange(0, 256))\n",
        "#     return entropy\n",
        "# def calculate_entropy(img):\n",
        "#     histogram = img.histogram()\n",
        "#     total_pixels = sum(histogram)\n",
        "#     probabilities = [count / total_pixels for count in histogram]\n",
        "#     entropy = -sum(p * math.log2(p) for p in probabilities if p > 0)\n",
        "#     return entropy\n",
        "\n",
        "def calculate_entropy(img):\n",
        "    image = np.array(img)\n",
        "    image = image / 255.0\n",
        "    entropy = shannon_entropy(image)\n",
        "    return entropy"
      ],
      "metadata": {
        "id": "hO6GZacsL2Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR1SElm50GH6"
      },
      "outputs": [],
      "source": [
        "def remove_by_entropy(origin_dataset, percent):\n",
        "    results = []\n",
        "    entropy_values = []\n",
        "    for item in origin_dataset:\n",
        "      entropy = calculate_entropy(item[0])\n",
        "      results.append([item, entropy])\n",
        "      entropy_values.append(entropy)\n",
        "      \n",
        "    results = sorted(results, key=lambda x: x[1])\n",
        "\n",
        "    count_to_remove = int((len(origin_dataset) * percent) / 100)\n",
        "    print(count_to_remove);\n",
        "    results = results[(count_to_remove - 1):]\n",
        "    return results, entropy_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ld-x7r9NNAR"
      },
      "outputs": [],
      "source": [
        "def balanced_removal_by_entropy(origin_dataset, percent):\n",
        "    results = []\n",
        "    entropy_values = []\n",
        "    for item in origin_dataset:\n",
        "        entropy = calculate_entropy(item[0])\n",
        "        results.append([item, entropy])\n",
        "        entropy_values.append(entropy)\n",
        "\n",
        "    results = sorted(results, key=lambda x: x[1])\n",
        "    # Cifar 100 dataset contains 500 data points for each class\n",
        "    count_to_remove = int((500 * percent) / 100)\n",
        "    class_removed_count = [0] * len(np.unique(origin_dataset.targets))\n",
        "    for item in results:\n",
        "        if class_removed_count[item[0][1]] < count_to_remove:\n",
        "            results.remove(item)\n",
        "            class_removed_count[item[0][1]] += 1\n",
        "    return results, entropy_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgSx_3IGNT1V"
      },
      "outputs": [],
      "source": [
        "removal_method = balanced_removal_by_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7RUTfNyw2c2"
      },
      "outputs": [],
      "source": [
        "def get_dataloader(trainset, testset, remove_images, percent = None):\n",
        "    transform_train = torchvision.transforms.Compose([\n",
        "      torchvision.transforms.RandomCrop(32, padding=4),\n",
        "      torchvision.transforms.RandomHorizontalFlip(),\n",
        "      torchvision.transforms.ToTensor(),\n",
        "      normalize\n",
        "    ])\n",
        "    transform_train.transforms.append(Cutout(n_holes=1, length=16))\n",
        "\n",
        "    if remove_images:  \n",
        "        trainset, entropy_values = removal_method(trainset, percent)\n",
        "\n",
        "        x_train =  [item[0][0] for item in trainset]\n",
        "        y_train =  [item[0][1] for item in trainset]\n",
        "    else:\n",
        "        x_train =  [item[0] for item in trainset]\n",
        "        y_train =  [item[1] for item in trainset]\n",
        "        entropy_values = None\n",
        " \n",
        "    train_dataset = Dataset(x_train, y_train, transform_train)\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False)\n",
        "    \n",
        "    return train_dataset, testloader, entropy_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8mU6aSb3qpt"
      },
      "outputs": [],
      "source": [
        "transform_test = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR100(root= 'data', train=True, download=True)\n",
        "testset = torchvision.datasets.CIFAR100(root= 'data', train=False, download=True, transform=transform_test)\n",
        "print('number of train images:' + str(len(trainset)))\n",
        "print('number of test images:' + str(len(testset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlyhQTb0SxZ2"
      },
      "outputs": [],
      "source": [
        "# From https://github.com/xternalz/WideResNet-pytorch\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.droprate = dropRate\n",
        "        self.equalInOut = (in_planes == out_planes)\n",
        "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
        "                               padding=0, bias=False) or None\n",
        "    def forward(self, x):\n",
        "        if not self.equalInOut:\n",
        "            x = self.relu1(self.bn1(x))\n",
        "        else:\n",
        "            out = self.relu1(self.bn1(x))\n",
        "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
        "        if self.droprate > 0:\n",
        "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
        "        out = self.conv2(out)\n",
        "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
        "        super(NetworkBlock, self).__init__()\n",
        "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
        "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
        "        layers = []\n",
        "        for i in range(int(nb_layers)):\n",
        "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n",
        "        super(WideResNet, self).__init__()\n",
        "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
        "        assert((depth - 4) % 6 == 0)\n",
        "        n = (depth - 4) / 6\n",
        "        block = BasicBlock\n",
        "        # 1st conv before any network block\n",
        "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        # 1st block\n",
        "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
        "        # 2nd block\n",
        "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
        "        # 3rd block\n",
        "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
        "        # global average pooling and classifier\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
        "        self.nChannels = nChannels[3]\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.bias.data.zero_()\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.relu(self.bn1(out))\n",
        "\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(-1, self.nChannels)\n",
        "        out = self.fc(out)\n",
        "        return out "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBV-aR26lspA"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model,data):\n",
        "    return model(data).cpu().numpy().argmax(axis=1)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHLvzznWMACt"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "def run_model(model, optimizer, learning_rate, epoch_num, train_dataset, testloader):\n",
        "    if train_on_gpu:\n",
        "       model.cuda()\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss().cuda()\n",
        "    criterion.__init__(reduce=False)\n",
        "    \n",
        "    if optimizer.__name__ == \"SGD\":\n",
        "        optimizer = optimizer(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True, weight_decay=5e-4)\n",
        "        lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60, 120, 160], gamma=0.2)\n",
        "    else:\n",
        "        optimizer = optimizer(model.parameters(), lr=learning_rate)  # for Adam\n",
        "\n",
        "    val_accs = np.zeros(epoch_num)  \n",
        "\n",
        "    lr_arr = []\n",
        "    p_bar = tqdm(range(epoch_num))\n",
        "\n",
        "    for i_epoch in p_bar:\n",
        "        model.train()\n",
        "\n",
        "        if isinstance(optimizer, torch.optim.SGD):\n",
        "            lr_arr.append(lr_scheduler.get_last_lr()[0])\n",
        "\n",
        "        trainset_permutation_inds = npr.permutation(np.arange(len(train_dataset.targets)))\n",
        "        for batch_idx, batch_start_ind in enumerate(range(0, len(train_dataset.targets), 128)):\n",
        "            # Get trainset indices for batch\n",
        "            batch_inds = trainset_permutation_inds[batch_start_ind: batch_start_ind + 128]\n",
        "            # Get batch inputs and targets, transform them appropriately\n",
        "            transformed_trainset = []\n",
        "            for ind in batch_inds:\n",
        "                transformed_trainset.append(train_dataset.__getitem__(ind)[0])\n",
        "            inputs = torch.stack(transformed_trainset)\n",
        "            targets = torch.LongTensor(np.array(train_dataset.targets)[batch_inds].tolist())\n",
        "            # Map to available device\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "            # Forward propagation, compute loss, get predictions\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Update loss, backward propagate, update optimizer\n",
        "            loss = loss.mean()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if isinstance(optimizer, torch.optim.SGD):\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        correct = 0.\n",
        "        total = 0.\n",
        "        with torch.no_grad():  # no need to calculate gradients when assessing accuracy\n",
        "            for i_batch, (X_val, Y_val) in enumerate(testloader):\n",
        "\n",
        "                if (train_on_gpu):\n",
        "                    X_val, Y_val = X_val.cuda(), Y_val.cuda()\n",
        "\n",
        "                model.eval()\n",
        "\n",
        "                pred_val = get_predictions(model, X_val)\n",
        "                total += X_val.size(0)\n",
        "                correct += (pred_val == Y_val.cpu().numpy()).sum()\n",
        "\n",
        "            val_acc = 100. * correct.item() / total\n",
        "            val_accs[i_epoch] = val_acc\n",
        "            p_bar.set_description((\"max accuracy: \" + str(val_accs.max()) + ' accuracy: ' + str(val_acc)),\n",
        "                                  refresh=True)\n",
        "        \n",
        "    return val_accs, lr_arr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrN2Fi1o0Zon"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(5)\n",
        "torch.cuda.manual_seed(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOLRLfCL0ivj",
        "outputId": "990a569a-22c9-42d7-ec17-4ba6c0591861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of remaining train images:47500\n",
            "number of remaining test images:10000\n",
            "removed images:5.0%\n"
          ]
        }
      ],
      "source": [
        "entropy_percent = 5\n",
        "train_dataset, testloader, entropy_values = get_dataloader(trainset, testset, True, entropy_percent)\n",
        "print('number of remaining train images:' + str(len(train_dataset)))\n",
        "print('number of remaining test images:' + str(len(testloader.dataset)))\n",
        "percent = ((len(trainset) - len(train_dataset)) * 100)/len(trainset) \n",
        "print('removed images:' + str(percent) + '%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-eizE5h0nO-"
      },
      "outputs": [],
      "source": [
        "wideResNet = WideResNet(depth=28, num_classes=100, widen_factor=10, dropRate=0.3) \n",
        "optimizer = torch.optim.Adam\n",
        "val_acc = run_model(wideResNet, optimizer, 0.001, 200, train_dataset, testloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKEEY7SM0qFs"
      },
      "outputs": [],
      "source": [
        "plt.plot(val_acc)  \n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Val Accuracy')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNXURcuy0sAW"
      },
      "outputs": [],
      "source": [
        "# Draw entropy histogram\n",
        "import math\n",
        "n = len(entropy_values)\n",
        "range_val = max(entropy_values) - min(entropy_values)\n",
        "root = int(math.sqrt(n));\n",
        "intervals_Width = range_val/root\n",
        "bins = [(min(entropy_values) + (x * intervals_Width)) for x in np.arange (0, root, 1)]\n",
        "plt.hist(entropy_values, bins=bins)\n",
        "plt.xlabel('Entropy Value')\n",
        "plt.ylabel('Number of images')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvGL1uo5O5Ev"
      },
      "outputs": [],
      "source": [
        "# Draw entropy histograms per class\n",
        "def get_entropies_by_class(dataset, class_num):\n",
        "  entropies = []\n",
        "  for item in dataset :\n",
        "     if item[1] == class_num:\n",
        "        entropy = calculate_entropy(item[0])\n",
        "        entropies.append(entropy)\n",
        "  return entropies\n",
        " \n",
        "import math\n",
        "\n",
        "for index in range(100):\n",
        "  entropy_values = get_entropies_by_class(trainset, index)\n",
        " \n",
        "  n = len(entropy_values)\n",
        "  range_val = max(entropy_values) - min(entropy_values)\n",
        "  root = int(math.sqrt(n));\n",
        "  intervals_Width = range_val/root\n",
        "  bins = [(min(entropy_values) + (x * intervals_Width)) for x in np.arange (0, root, 1)]\n",
        "  plt.hist(entropy_values, bins=bins)\n",
        "  plt.xlabel('class: ' + str(index))\n",
        "  plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}